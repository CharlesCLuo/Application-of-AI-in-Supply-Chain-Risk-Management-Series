{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvWvXbVOKCq+HEeksYSxgU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pDXfqMZ0MVow"},"outputs":[],"source":["!pip install requests\n","!pip install torch\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import requests\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Adds positional information to the input sequence.\n","    This helps the Transformer model understand the order of data points in the sequence.\n","    \"\"\"\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:x.size(0), :]\n","\n","class TransformerModel(nn.Module):\n","    \"\"\"\n","    A Transformer model for sequence prediction. This model predicts the next value in a sequence based on past values.\n","    \"\"\"\n","    def __init__(self, input_size=1, hidden_size=64, num_layers=2, nhead=8, max_len=500):\n","        super(TransformerModel, self).__init__()\n","        self.embedding = nn.Linear(input_size, hidden_size)\n","        self.pos_encoder = PositionalEncoding(hidden_size, max_len=max_len)\n","        self.transformer = nn.Transformer(d_model=hidden_size, nhead=nhead,\n","                                          num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, src):\n","        src = self.embedding(src)\n","        src = src.permute(1, 0, 2)\n","        src = self.pos_encoder(src)\n","        transformer_out = self.transformer(src, src)\n","        output = self.fc(transformer_out[-1, :, :])\n","        return output\n","\n","\n","def load_data_from_github(url):\n","    \"\"\"\n","    Downloads and loads data from a GitHub URL. The data should be in .pt format.\n","    \"\"\"\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    data = torch.load(BytesIO(response.content))\n","    return data\n","\n","\n","url = \"https://github.com/CharlesCLuo/Application-of-AI-in-Supply-Chain-Risk-Management-Series/blob/main/Demand_Forecsting/transformer_data.pt?raw=true\"\n","\n","\n","data = load_data_from_github(url)\n","\n","\n","sequence1 = data['sequence1']\n","sequence2 = data['sequence2']\n","\n","\n","sequence1_df = pd.DataFrame(sequence1.numpy().round(2), columns=['Sequence 1'])\n","sequence2_df = pd.DataFrame(sequence2.numpy().round(2), columns=['Sequence 2'])\n","\n","\n","print(\"First 5 rows of Sequence 1:\")\n","print(sequence1_df.head().to_string(index=False))\n","print(\"\\nFirst 5 rows of Sequence 2:\")\n","print(sequence2_df.head().to_string(index=False))\n","\n","print(\"\\nDescriptive Statistics for Sequence 1:\")\n","print(sequence1_df.describe().round(2))\n","print(\"\\nDescriptive Statistics for Sequence 2:\")\n","print(sequence2_df.describe().round(2))\n","\n","\n","def split_sequence(sequence, train_ratio=0.7):\n","    \"\"\"\n","    Splits the sequence into training and validation sets based on the train_ratio.\n","    \"\"\"\n","    train_size = int(len(sequence) * train_ratio)\n","    train_sequence = sequence[:train_size]\n","    val_sequence = sequence[train_size:]\n","    return train_sequence, val_sequence\n","\n","train_seq1, val_seq1 = split_sequence(sequence1, train_ratio=0.7)\n","train_seq2, val_seq2 = split_sequence(sequence2, train_ratio=0.7)\n","\n","\n","def create_sliding_window_sequences(sequence, window_size=20):\n","    \"\"\"\n","    Converts a sequence into sliding windows of input-output pairs.\n","    The input is a sequence of 'window_size' values, and the output is the next value.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(sequence) - window_size):\n","        X.append(sequence[i:i + window_size])\n","        y.append(sequence[i + window_size])\n","    return torch.stack(X), torch.stack(y)\n","\n","window_size = 20\n","X_train_seq1, y_train_seq1 = create_sliding_window_sequences(train_seq1, window_size=window_size)\n","X_val_seq1, y_val_seq1 = create_sliding_window_sequences(val_seq1, window_size=window_size)\n","\n","X_train_seq2, y_train_seq2 = create_sliding_window_sequences(train_seq2, window_size=window_size)\n","X_val_seq2, y_val_seq2 = create_sliding_window_sequences(val_seq2, window_size=window_size)\n","\n","\n","def train_model(model, X_train, y_train, X_val, y_val, num_epochs=50, lr=0.0001):\n","    \"\"\"\n","    Trains the Transformer model using the training dataset and evaluates on the validation dataset.\n","    \"\"\"\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    train_losses, val_losses = [], []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train)\n","        loss = criterion(outputs, y_train)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val)\n","            val_loss = criterion(val_outputs, y_val)\n","            val_losses.append(val_loss.item())\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n","\n","    return model, train_losses, val_losses\n","\n","\n","transformer_model_seq1 = TransformerModel(max_len=500)\n","trained_model_seq1, train_losses_seq1, val_losses_seq1 = train_model(transformer_model_seq1, X_train_seq1, y_train_seq1, X_val_seq1, y_val_seq1)\n","\n","\n","transformer_model_seq2 = TransformerModel(max_len=500)\n","trained_model_seq2, train_losses_seq2, val_losses_seq2 = train_model(transformer_model_seq2, X_train_seq2, y_train_seq2, X_val_seq2, y_val_seq2)\n","\n","\n","def evaluate_metrics(y_true, y_pred, name):\n","    \"\"\"\n","    Computes and displays evaluation metrics: MSE, MAE, and R² Score.\n","    \"\"\"\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    print(f\"\\n{name} Metrics:\")\n","    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","    print(f\"R² Score: {r2:.4f}\")\n","\n","\n","trained_model_seq1.eval()\n","with torch.no_grad():\n","    val_predictions_seq1 = trained_model_seq1(X_val_seq1).squeeze().numpy()\n","    actual_values_seq1 = y_val_seq1.squeeze().numpy()\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(actual_values_seq1, label=\"Actual (Validation Sequence 1)\", color='black')\n","plt.plot(val_predictions_seq1, label=\"Predicted (Validation Sequence 1)\", color='red')\n","plt.title(\"Transformer Predictions vs Actual Values (Sequence 1)\")\n","plt.xlabel(\"Sample\")\n","plt.ylabel(\"Value\")\n","plt.legend()\n","plt.show()\n","\n","evaluate_metrics(actual_values_seq1, val_predictions_seq1, \"Sequence 1\")\n","\n","\n","trained_model_seq2.eval()\n","with torch.no_grad():\n","    val_predictions_seq2 = trained_model_seq2(X_val_seq2).squeeze().numpy()\n","    actual_values_seq2 = y_val_seq2.squeeze().numpy()\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(actual_values_seq2, label=\"Actual (Validation Sequence 2)\", color='black')\n","plt.plot(val_predictions_seq2, label=\"Predicted (Validation Sequence 2)\", color='blue')\n","plt.title(\"Transformer Predictions vs Actual Values (Sequence 2)\")\n","plt.xlabel(\"Sample\")\n","plt.ylabel(\"Value\")\n","plt.legend()\n","plt.show()\n","\n","evaluate_metrics(actual_values_seq2, val_predictions_seq2, \"Sequence 2\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Rw8buWzTOSEa"},"execution_count":null,"outputs":[]}]}